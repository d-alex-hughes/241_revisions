{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "library(data.table)\n",
    "library(lmtest)\n",
    "library(sandwich)\n",
    "library(stargazer)\n",
    "options(repr.plot.width=4, repr.plot.height=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Teacher Incentives and Student Outcomes \n",
    "This data is from a really interesting paper written by [Karthik Muralidharan and Venkatesh Sundararaman](https://www.jstor.org/stable/10.1086/659655?seq=1#metadata_info_tab_contents) about how teachers respond to pay incentives in India. \n",
    "\n",
    "Here is the abstract from the paper: \n",
    "\n",
    "> We present results from a randomized evaluation of a teacher performance pay program implemented across a large representative sample of government-run rural primary schools in the Indian state of Andhra Pradesh. At the end of 2 years of the program, students in incentive schools performed significantly better than those in control schools by 0.27 and 0.17 standard deviations in math and language tests, respectively. We find no evidence of any adverse consequences of the program. The program was highly cost effective, and incentive schools performed significantly better than other randomly chosen schools that received additional schooling inputs of a similar value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = fread('./performance_pay_replication/data/dta/Incentives_JPE_HTEs.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a lot of data in this data frame and the authors have shared all the data they colleted. Lets focus on the following variables only: \n",
    "\n",
    "- `cheaters_y2`: people who cheated in the study window;\n",
    "  we don't want their data\n",
    "- `y2_nts_level_mean`: the DV, a standardized \n",
    "- `y0_nts`: the individual's year prior national test score\n",
    "- `incentive`: a treatment indicator -- was the school in control or any treatment \n",
    "- `school_treatment`: a treatment indicator -- was the school in control, a group treatment, or an individual treatment\n",
    "- `parent_literacy_index`: the 1-4 indicators for parental literacy\n",
    "- `hh_affluence_index`: the 1-7 affluence index \n",
    "- `U_MC`: the mandal\n",
    "- `apfschoolcode`: the school code (which we will eventually use\n",
    "  to cluster the standard errors.\n",
    "  \n",
    "The rest of the data, as well as their codebook, and analysis files are available [here](link). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d[1:10, .(cheaters_y2, y2_nts_level_mean, y0_nts, incentive, \n",
    "          parent_literacy_index, hh_affluence_index, U_MC, apfschoolcode)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d[ , table(school_treatment, incentive)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis\n",
    "Before beginning your analysis of the experiment, *per se*, check to see if there are any strange or outlying values in these data fields. At the same time, check that you understand the level of information that is coded in each data field (e.g. interval, ratio, categorial) and how it is distributed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d[ , table(cheaters_y2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "histogram <- d[ , hist(y2_nts_level_mean, col = 'black')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make a determination about what you want to do for missingness across the dataset. There is a _lot_ of itemwise missingness -- people who weren't at school the day of the tests, for example. \n",
    "\n",
    "Although it isn't generally a sound practice, for the concision of this code, **go ahead and drop these observations**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_no_na <- na.omit(d, cols = c('y2_nts_level_mean', 'y0_nts', 'cheaters_y2'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis of Experiment \n",
    "We'll write the first regression for you, but the rest are up to you! \n",
    "\n",
    "Let's focus only on those individuals who didn't cheat in the second period. Among this set of people who fairly took the test, what is the causal effect of having a teacher who was a part of any incentive program? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod_overall <- d_no_na[ ,  lm(y2_nts_level_mean ~ incentive + y0_nts + factor(U_MC))]\n",
    "test_results <- coeftest(mod_overall, vcovCL(mod_overall, d_no_na[ , apfschoolcode]))\n",
    "\n",
    "test_results[1:4, ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a clear effect of teachers being in any of the incentive conditions. Students whose teachers were in any incentive condition scores 0.24 standard deviations higher than students whose teachers were not in that condition. \n",
    "\n",
    "**What is happening in that call?**\n",
    "- First, we're filtering the rows to incldue only thos who don't cheat and who don't have a NA in the outcome value\n",
    "- Second, we're transforming the column space by a *regression transform* -- we can do anything to the columns, including a regression! \n",
    "- Third, we're using the `coeftest` package which will pretty-print coefficients and standard errors for us; the standard errors that we are calculating are clustered standard errors, since the treatment assignment happens at the *school level*. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Different Types of Treatment? \n",
    "\n",
    "Using a similar regression set up as above, are the effects of being in different *types* of treatment meaningfully different from one another? That is, use a regression of the variable `school_treatment` to measure the effects of the different types of treatment.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod_treatments <- d_no_na[ , lm(y2_nts_level_mean ~ school_treatment)]\n",
    "# fill out the standard errors calculation and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To the heart of the question: Parental Literacy\n",
    "\n",
    "Using a similar regression set up as above, test for whether receiving any incentives (`incentive`) operates differently when student's parents score have different scores on `parents_literacy_index`. \n",
    "\n",
    "> The theory underlying this test is that when teachers are provided incentives to teach more, perhaps these incentives are particularily well-received when the students' home-lives also support learning. \n",
    "\n",
    "Because of the way that `parents_literacy_index` is coded, you should probably use this as a factor variable, and test appropriately for heterogeneity across this multi-level factor varible using an F-test first. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod_short <- ''\n",
    "mod_long <- ''\n",
    "\n",
    "anova(mod_short, mod_long, test = 'F')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stargazer(mod_short, mod_long, type = 'text')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do you conclude about the effectiveness of these treatments? Do they work differently when supported by parents who read well than parents who do not read well? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To the heart of the question: Household Affluence \n",
    "\n",
    "Using a similar regression set up as above, test for whether receiving any incentives (`incentive`) operates differently when student's parents score have different scores on `hh_affluence_index`. \n",
    "\n",
    "> The theory underlying this test is that when teachers are provided incentives to teach more, perhaps these incentives are particularily well-received when the students' home-lives also support learning. \n",
    "\n",
    "Because of the way that `parents_literacy_index` is coded, you should probably use this as a factor variable, and test appropriately for heterogeneity across this multi-level factor varible using an F-test first. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do you conclude about the effectiveness of these treatments? Do they work differently when supported by households that are relatively wealthy rather than relatively poor? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "library(data.table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On and around page 304 of _Field Experiments_ Green & Gerber give us a great example to work with to consider multi-factor experiments -- the case of \"Colin\" and \"Jose\" sending letters to their elected representatives. \n",
    "\n",
    "> This is actually a classic example of an audit experiment -- see for example the work of [Devah Pager](https://scholar.harvard.edu/files/pager/files/pager_ajs.pdf) who used audit studies to evalute the penalty suffered by black job applicants in the labor market. \n",
    "\n",
    "The thing about the example is that Green and Gerber have provided us the data in a way that it has already been summarized for us. This is both (a) common to receive a table that has already been summarized; and (b) frustrating because we can't actually do any work against the data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not only have Green and Gerber summarized the data for us, but they have presented it in four columns and a single row. This hides the very clear 2x2 structure of the experiment! \n",
    "\n",
    "Write out the 4x1 table as a 2x2 table in the markdown cell below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|             | Colin | Jose | Total |\n",
    "|-------------|-------|------|-------|\n",
    "| Good Grammar|       |      |       |\n",
    "| Bad Grammar |       |      |       |\n",
    "| **Total**   |       |      |       |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After writing down what the conditions are, can you also make data that would produce this table? \n",
    "\n",
    "- What is the outcome that they are evaluating, and what are the possible values for this outcome? How should you encode this information? \n",
    "- How many observations are there for each of the cells in this experiment? \n",
    "- How can you represent the percentages that you've written down in each cell? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d <- data.table(\n",
    "    'id'      = 1:N, # change N here to the total number of observations,\n",
    "    'name'    = rep(), # fill in with name info \n",
    "    'grammar' = rep(), # fill in with grammar info\n",
    "    ) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you've got the experiment structured, then you can start to build out the outcome you identified above. We'll create a variable name -- `response` -- for you and set all the values to 0. \n",
    "\n",
    "Select rows of the data.table, and fill in the response variable as appropriate to complete the table. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d[ , response := 0L]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you've built your data correctly, then you should be able to run the next line, and return back out the 2x2 table that you wrote down earlier. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d[ , .(response_rate = mean(response)), by = .(name, grammar)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> In the check above, you are creating a new variable, called `response_rate` that is the avarage of the responses within each of the name-grammar conditions. Those name-grammar conditions are called out in the `... by = .(name, grammar)` argument. \n",
    "\n",
    "What you're showing above is equivalent to estimating a \"saturated\" model where you have as many indicators for the condition as you do have conditions. These saturated models will report the means within each of the condition groups. \n",
    "\n",
    "One way to estimate a saturated model is to suppress the intercept -- using a -1 call (read, \"not intercept\") -- and then include the indicators for the features. For example, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saturated_model_1 = d[ , lm(response ~ -1 + name + grammar)]\n",
    "summary(saturated_model_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The other way to estimate this saturated model with a regression is to include an indicator for each of the name-grammar conditions. \n",
    "\n",
    "Produce a new column on this data.table called `treatment_condition` that has an indicator for the name-grammar condition, and then use this single feature in a model to estimate the saturated indicator. (Don't just hard code this based on position in the data.table; instead, think about writing the vectorized conditional operation.) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d[name=='' & grammar=='', treatment_condition := ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saturated_model_2 = d[ , lm(response ~ treatment_condition)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take care to interpret what you're getting back from this model.\n",
    "\n",
    "1. Does it produce the results that you thought it would? Why or why not? If not, what change could you make to the model to make it produce what you anticipated it would? \n",
    "2. *Specifically* what tests are being reported? What is the null hypothesis for each of these tests? Is this a meaningful test? Why or why not? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Estimate Model 9.12\n",
    "\n",
    "Despite the ease of estimating the *saturated model*, it is possible to structure your model so that the sets of tests reported are more useful to you. In the model described as model 9.12 in _Field Experiments_ Green & Gerber write down a model that is: \n",
    "\n",
    "$$\n",
    "Y_{i} = \\beta_{0} + \\beta_{1} Name + \\beta_{2} Gramar + \\beta_{3} Name \\times Grammar\n",
    "$$ \n",
    "\n",
    "Estimate this model in the next cell and store the results as an object `interaction_model`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interaction_model = d[ , lm()]\n",
    "interaction_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspect the results of this model: \n",
    "1. What are each of the coefficients reporting? \n",
    "2. Which coefficient or coefficients in this model have an exactly identical coefficient in the `saturated_model`. What does this mean?  Why are the other coefficients different?\n",
    "3. How can you combine sets of coefficients to contain the same information as in the earlier model? \n",
    "4. What are the tests that are being reported for each of these coefficients? \n",
    "  1. Is the null hypothesis in this model for each of the coefficients the same as before? Why or why not? \n",
    "  2. Are you surprised that fewer coefficients are significant in this model? Why or why not? \n",
    "  3. Which set of null hypotheses -- those in the `saturated_model` or in this `interaction_model` more closely match with the tests that you care about? Why? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
